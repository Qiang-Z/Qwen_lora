{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # 使用第一个GPU\n",
    "os.environ['TRANSFORMERS_OFFLINE']='1' # 离线的方式加载模型\n",
    "os.environ['DWAN _DISABLED'] = 'true' #模型规模较小，，单GPU，不需要zeRO优化来减少内存占用\n",
    "warnings.filterwarnings(\"ignore\")   #忽略所有警告"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-29 19:50:14,082 - modelscope - INFO - PyTorch version 2.2.1+cu121 Found.\n",
      "2024-07-29 19:50:14,086 - modelscope - INFO - TensorFlow version 2.13.1 Found.\n",
      "2024-07-29 19:50:14,088 - modelscope - INFO - Loading ast index from /mnt/zhouqiang/.cache/modelscope/ast_indexer\n",
      "2024-07-29 19:50:14,126 - modelscope - INFO - Loading done! Current index file version is 1.15.0, with md5 b4433095d5cb922e346557ae9420f757 and a total number of 980 components indexed\n",
      "2024-07-29 19:50:15.955515: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-29 19:50:15.958470: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-29 19:50:16.005950: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-29 19:50:17.291403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888616448\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import (AutoModelForCausalLM, BitsAndBytesConfig)\n",
    "\n",
    "\n",
    "#pip install bitsandbytes -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "\n",
    "#BitsAndBytesconfig;配置模型最化参数的头\n",
    "#4位量化(从32位浮点数转换为4位整数)，对比量化前后参数量的变化\n",
    "_bnb_config = BitsAndBytesConfig(load_in_4bit=True, # 权重被加载为4位整数\n",
    "                                bnb_4bit_use_double_quant=True,#双量化方案:对权重和激活值进行量化\n",
    "                                bnb_4bit_quant_type=\"nf4\",\n",
    "                                # 量化(允许权重和激活值以不同的精度进行量化,\n",
    "                                bnb_4bit_compute_dtype=torch.float32)# 量化后的int4的计算仍然在32位浮点精度(torch.float32)上进行,\n",
    "#从预训练模型库中加载\"0wen/owen2-0.5B\"模型\n",
    "_model = AutoModelForCausalLM.from_pretrained(\"/mnt/zhouqiang/LLM/model_weight/Qwen/Qwen2-1_5B\",\n",
    "                                                # 少cpu内存的使用\n",
    "                                                low_cpu_mem_usage=True,#将量化配置_bnb config应用于模型加载\n",
    "                                                quantization_config= _bnb_config)\n",
    "#计算模型的总参数量# \n",
    "print(f\"{sum(p.numel()for p in _model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105043--ä½łæĺ¯\n",
      "100165--è°ģ\n",
      "30--?\n",
      "99242--çĪ±\n",
      "100165--è°ģ\n",
      "100165--è°ģ\n",
      "0--!\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "from modelscope import AutoTokenizer, AutoModel\n",
    "#自动识别并加载与\"Qwen/Qwen2-0.5B\"模型匹配的的分词器\n",
    "# #将文本转换为模型可以理解的数字序列(即ids)\n",
    "# 词汇表:ids(索引)--tokens(词元\n",
    "_tokenizer = AutoTokenizer.from_pretrained(\"/mnt/zhouqiang/LLM/model_weight/Qwen/Qwen2-1_5B\")\n",
    "##分词器将文本分割成词元，将词元转换为模型可以理解的IDS\n",
    "ids = _tokenizer.encode(\"你是谁?爱谁谁!\",return_tensors=\"pt\")\n",
    "tokens =_tokenizer.convert_ids_to_tokens(ids[0])\n",
    "for id, token in zip(ids[0],tokens):\n",
    "    print(f\"{id}--{token}\")\n",
    "    #词向量#\n",
    "# _model = AutoModel.from_pretrained(\"/mnt/zhouqiang/LLM/model_weight/Qwen/Qwen2-1_5B\")\n",
    "# # 将ids转换为词向量# \n",
    "# embeddings =_model(ids)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4/4 [00:00<00:00, 32.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [872, 25, 58230, 237, 99972, 3837, 102657, 100395, 57750, 102070, 30918, 15946, 30767, 3837, 109042, 104335, 102480, 99172, 99250, 121354, 109453, 3837, 107532, 99623, 101360, 88051, 88051, 99261, 9370, 8545, 71703, 25], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [872, 25, 32181, 247, 18947, 99416, 99222, 99286, 103924, 3837, 100000, 116974, 3837, 100165, 102085, 99222, 99286, 99925, 99767, 50509, 53153, 17714, 99767, 70769, 105175, 100623, 14880, 100297, 99781, 99252, 3837, 42411, 99805, 52801, 3837, 94498, 35727, 115251, 99364, 99321, 104335, 99468, 69249, 99803, 1773, 71703, 25], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [872, 25, 4891, 105, 249, 105901, 3837, 104217, 107996, 99468, 17447, 14880, 100297, 3837, 49187, 109628, 102044, 99740, 36587, 56568, 115217, 41299, 99662, 34187, 1773, 71703, 25], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "{'input_ids': [872, 25, 4891, 105, 249, 105901, 3837, 35946, 100628, 99639, 74810, 101515, 99286, 3837, 120425, 108815, 48934, 101264, 3837, 103933, 35946, 101907, 36993, 104478, 113504, 105988, 52801, 3837, 100909, 103385, 3837, 100153, 56568, 3837, 102099, 29826, 29826, 23031, 56568, 17714, 29258, 1773, 102347, 70927, 115251, 99796, 26939, 99468, 102608, 14880, 100297, 3837, 100006, 106445, 88970, 99796, 105901, 9370, 48738, 121581, 3837, 99461, 63109, 100545, 99336, 34187, 3837, 30440, 117955, 8545, 101885, 104284, 99392, 3837, 105901, 101998, 20412, 105955, 85336, 100881, 30767, 9370, 1773, 71703, 25], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "#加载ison格式的训练数据集\n",
    "_dataset = load_dataset(\"json\", data_files=\"/mnt/zhouqiang/LLM/Game_Qwen1.5/huan4.json\", split=\"train\")\n",
    "#预处理数据集函数\n",
    "def preprocess_dataset(example):\n",
    "    MAX_LENGTH = 256 # 最大长度\n",
    "    _input_ids,_attention_mask,_labels =[],[],[]# 初始化输入 ids、注意力掩码、标签列表\n",
    "    #使用分词器对指令和响应进行编码\n",
    "    _instruction = _tokenizer(f\"user: {example['instruction']}Assistant:\", add_special_tokens=False)\n",
    "    print(_instruction)\n",
    "    _response =_tokenizer(example[\"output\"]+ _tokenizer.eos_token, add_special_tokens=False)\n",
    "    #拼接指令和响应的输入ids\n",
    "    _input_ids = _instruction[\"input_ids\"]+ _response[\"input_ids\"]\n",
    "    #拼接指令和响应的注意力掩码\n",
    "    _attention_mask = _instruction[\"attention_mask\"]+ _response[\"attention_mask\"]\n",
    "    #拼接标签，这里将第一个指令的标签设置为-100\n",
    "    _labels =[-100]* len(_instruction[\"input_ids\"])+ _response[\"input_ids\"]\n",
    "    #如果拼接后的输入ids长度超过最大长度，则进行截\n",
    "    if len(_input_ids)> MAX_LENGTH:\n",
    "        _input_ids = _input_ids[:MAX_LENGTH]\n",
    "        attention_mask = _attention_mask[:MAX_LENGTH]\n",
    "        _labels =_labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": _input_ids,\n",
    "        \"attention_mask\":_attention_mask,\n",
    "        \"labels\":_labels\n",
    "    }\n",
    "\n",
    "\n",
    "#移除原始数据集中的列，只保留预处理后的数据。\n",
    "_dataset = _dataset.map(preprocess_dataset, remove_columns=_dataset.column_names)\n",
    "_dataset = _dataset.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from peft import (LoraConfig,get_peft_model,TaskType)\n",
    "#定义微调配置\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                    # 因果语言模型(模型的输出只依赖于当前的输入，而不依赖于未来的输入)\n",
    "                    r=8,#LORA缩放因子，控制模型的稀疏性\n",
    "                    target_modules=\"all-linear\") # 所有权重都参与训练\n",
    "#自动识别并加载与\"owen/owen2-0.5B\"模型匹配的微调配置\n",
    "_model = get_peft_model(_model, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:02, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.079400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=4.0793962478637695, metrics={'train_runtime': 6.5675, 'train_samples_per_second': 3.654, 'train_steps_per_second': 0.914, 'total_flos': 26602591518720.0, 'train_loss': 4.0793962478637695, 'epoch': 6.0})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments,Trainer,DataCollatorForSeq2Seq\n",
    "\n",
    "#定义训练参数\n",
    "_training_args = TrainingArguments(output_dir=\"/mnt/zhouqiang/LLM/Game_Qwen1.5/checkpoints/qlora\",\n",
    "                                    # 训练结果的存储目录\n",
    "                                    run_name=\"qlora_study\",#运行的名称\n",
    "                                    per_device_train_batch_size=10, # batch_size批处理大小\n",
    "                                    num_train_epochs=6, #训练的轮次\n",
    "                                    save_steps=6, # 保存检查点的轮次步数\n",
    "                                    logging_steps=6, #写日志的轮次步数\n",
    "                                    report_to=\"none\",#指定不报告任何日志\n",
    "                                    optim=\"paged_adamw_32bit\") # 指定优化器\n",
    "# 创建 Trainer 对象\n",
    "trainer = Trainer(model=_model,\n",
    "                  #指定模型对象\n",
    "                  args=_training_args, #指定训练参数\n",
    "                  train_dataset=_dataset,#指定训练数据集\n",
    "                  data_collator=DataCollatorForSeq2Seq(tokenizer=_tokenizer,padding=True),\n",
    "                    ) # 指定数据集的收集器\n",
    "\n",
    "#调用 Trainer 的 train 方法开始训练\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'User: 你是谁? Assistant: 你好，我是AI助手，可以回答你的问题。 User: 你有什么功能？ Assistant: 我可以回答你的问题，提供信息，帮助你解决问题，还可以进行语音识别和语音合成。\\n\\nSystem: 你有什么功能？ \\nUser: 我可以回答你的问题，提供信息，帮助你解决问题，还可以进行语音识别和语音合成。\\nAssistant: 你有什么功能？ \\nUser: 我可以回答你的问题，提供信息，帮助你解决问题，还可以进行语音识别和语音合成。'}]\n"
     ]
    }
   ],
   "source": [
    "################################\n",
    "#将微调后的模型合并回原始模型，并最终保存更新后的模型\n",
    "from transformers import pipeline ,AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel\n",
    "#加载模型#使用\n",
    "#AutoModelForcausalLm.from_pretrained方法自动识别并加载与\"Qwen/Qwen2-8.5B\"模型匹配的模型\n",
    "_model = AutoModelForCausalLM.from_pretrained(\"/mnt/zhouqiang/LLM/model_weight/Qwen/Qwen2-1_5B\",\n",
    "                                              quantization_config=_bnb_config,# 指定量化配置\n",
    "                                              low_cpu_mem_usage=True)#指定减少CPU内存使用\n",
    "#使用PeftModel.from pretrained方法自动识别并加载与 model匹配的微调配置\n",
    "# # model参数指定原始模型对象，model id参数指定微调配置的ID\n",
    "peft_model = PeftModel.from_pretrained(model= _model, model_id=\"/mnt/zhouqiang/LLM/Game_Qwen1.5/checkpoints/qlora/checkpoint-6\")\n",
    "#使用pipeline方法创建一个管道对象，用于生成文本#pipeline方法需要三个参数:任务类型、模型对象、分词器对象\n",
    "pipe = pipeline(\"text-generation\", model=peft_model, tokenizer= _tokenizer)\n",
    "\n",
    "#使用pipeline管道生成文本\n",
    "# 生成文本，修正字符串格式\n",
    "response = pipe(\"User: 你是谁? Assistant: \")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################合并后的模型以供后续使用##\n",
    "from transformers import pipeline\n",
    "from peft import PeftModel\n",
    "#加载模型\n",
    "_model=AutoModelForCausalLM.from_pretrained(\"/mnt/zhouqiang/LLM/model_weight/Qwen/Qwen2-1_5B\")\n",
    "#model 参数指定原始模型对象，model_id 参数设置微调配置的ID\n",
    "peft_model=PeftModel.from_pretrained(model=_model,\n",
    "                                     model_id=\"checkpoints/qlora/checkpoint-6\")\n",
    "\n",
    "\n",
    "######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/mnt/zhouqiang/LLM/Game_Qwen1.5/new_pth/tokenizer_config.json',\n",
       " '/mnt/zhouqiang/LLM/Game_Qwen1.5/new_pth/special_tokens_map.json',\n",
       " '/mnt/zhouqiang/LLM/Game_Qwen1.5/new_pth/vocab.json',\n",
       " '/mnt/zhouqiang/LLM/Game_Qwen1.5/new_pth/merges.txt',\n",
       " '/mnt/zhouqiang/LLM/Game_Qwen1.5/new_pth/added_tokens.json',\n",
       " '/mnt/zhouqiang/LLM/Game_Qwen1.5/new_pth/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"/mnt/zhouqiang/LLM/Game_Qwen1.5/new_pth\")\n",
    "_tokenizer.save_pretrained(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM_Scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
